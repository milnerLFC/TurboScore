{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@milnerLFC --ben rabat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import  accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# from ann_output import binary_ann_df\n",
    "\n",
    "from utils import list_seasons, custom_accuracy_margin, accuracy_double_strict, accuracy_double_margin, confusion_classification\n",
    "from download_data import download_league_season,download_fixtures,load_data,load_fixtures\n",
    "from ann_utils import split_data, scale_datasets\n",
    "\n",
    "# from mlp_classifier import mlp_model_fit\n",
    "\n",
    "from predictables import table, table_bet, table_filter, table_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = 15\n",
    "bet_amount_base = 10\n",
    "min_entropy = 0.5\n",
    "max_entropy = 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = datetime.datetime.now().year\n",
    "current_season = f\"{current_year%100:02d}{current_year%100+1:02d}\"\n",
    "key = \"E0\"\n",
    "leagues_keys = [key,\"E1\",\"SC0\",\"SP1\",\"F1\",\"D1\",\"I1\"]\n",
    "base_path = os.getcwd()\n",
    "folder_path = os.path.join(base_path, \"resources\", \"seasons\")\n",
    "\n",
    "existing_csv = {\n",
    "    entry.name for entry in os.scandir(folder_path)\n",
    "    if entry.is_file() and entry.name.endswith(\".csv\") and any(key in entry.name for key in leagues_keys)\n",
    "}\n",
    "\n",
    "venue_infos = ['Div','Date','Time','HomeTeam','AwayTeam']\n",
    "odds = ['PSH','PSD','PSA']\n",
    "columns_to_check = venue_infos + odds\n",
    "result_cols = ['FTHG','FTAG','FTR'] #'HTR','HTHG','HTAG'\n",
    "shot_cols = ['HS','AS','HST','AST']\n",
    "card_cols = ['HR','AR']\n",
    "usecols = venue_infos + result_cols + shot_cols + card_cols + odds\n",
    "unique_key = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"fixtures.csv\" \n",
    "download_fixtures(os.path.join(base_path, \"resources\"), filename)\n",
    "fixtures = load_fixtures(base_path, filename, columns_to_check, current_season, leagues_keys, unique_key=unique_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seasons = list_seasons(first_year=first_year, last_year=current_year)\n",
    "\n",
    "for season in all_seasons:\n",
    "    if unique_key:\n",
    "        filename = f\"{key}_{season}.csv\" \n",
    "        if (filename not in existing_csv) or season == current_season:\n",
    "            download_league_season(folder_path,filename,season,key)\n",
    "    else:\n",
    "        for k in leagues_keys:\n",
    "            filename = f\"{k}_{season}.csv\" \n",
    "            if (filename not in existing_csv) or season == current_season:\n",
    "                download_league_season(folder_path,filename,season,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_data = load_data(folder_path, all_seasons, usecols, leagues_keys, unique_key=unique_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_data['FTGD'] = league_data['FTHG'] - league_data['FTAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(league_data.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=\".1f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.15\n",
    "shift_val = 1\n",
    "win_size = 4\n",
    "seasons = league_data.groupby(\"Season\")\n",
    "\n",
    "Hcols = ['HPointsH','HGFH','HGCH','HSH','HSTH']\n",
    "Acols = ['APointsA','AGFA','AGCA','ASA','ASTA']\n",
    "\n",
    "seasonal_df = pd.DataFrame()\n",
    "columns_to_check.insert(1,'Season')\n",
    "for season_label, season_df in seasons:    \n",
    "    \n",
    "    if season_label == current_season:\n",
    "        # season_df = pd.concat([season_df, fixtures])\n",
    "        temp_seas_merged_df = pd.merge(season_df, fixtures, on=columns_to_check, how='outer', indicator=True)\n",
    "        unique_rows = temp_seas_merged_df[temp_seas_merged_df['_merge'] == 'right_only'].drop('_merge', axis=1)\n",
    "\n",
    "        season_df = pd.concat([season_df, unique_rows], ignore_index=True)\n",
    "        season_df = season_df.drop_duplicates(subset=columns_to_check, keep='first')\n",
    "\n",
    "        \n",
    "    # Home team statistics\n",
    "    season_df[['HGFH_m', 'HGCH_m']] = season_df.groupby('HomeTeam', group_keys=False)[['FTHG', 'FTAG']].apply(lambda x: x.shift(shift_val).expanding().mean())\n",
    "    season_df['HPointsH'] = season_df.apply(lambda row: 3 if row['FTR'] == 'H' else 1 if row['FTR'] == 'D' else 0, axis=1)\n",
    "    season_df['HPtsH_m'] = season_df.groupby('HomeTeam', group_keys=False)['HPointsH'].apply(lambda x: x.shift(shift_val).expanding().mean())\n",
    "    season_df[['HSH_m', 'HSTH_m']] = season_df.groupby('HomeTeam', group_keys=False)[['HS', 'HST']].apply(lambda x: x.shift(shift_val).expanding().mean())\n",
    "    # season_df['HPtsH_e'] = season_df.groupby('HomeTeam', group_keys=False)['HPointsH'].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "    \n",
    "    # Away team statistics\n",
    "    season_df[['AGFA_m', 'AGCA_m']] = season_df.groupby('AwayTeam', group_keys=False)[['FTAG', 'FTHG']].apply(lambda x: x.shift(shift_val).expanding().mean())\n",
    "    season_df['APointsA'] = season_df.apply(lambda row: 3 if row['FTR'] == 'A' else 1 if row['FTR'] == 'D' else 0, axis=1)\n",
    "    season_df['APtsA_m'] = season_df.groupby('AwayTeam', group_keys=False)['APointsA'].apply(lambda x: x.shift(shift_val).expanding().mean())\n",
    "    season_df[['ASA_m', 'ASTA_m']] = season_df.groupby('AwayTeam', group_keys=False)[['AS', 'AST']].apply(lambda x: x.shift(shift_val).expanding().mean())\n",
    "    # season_df['APtsA_e'] = season_df.groupby('AwayTeam', group_keys=False)['APointsA'].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "\n",
    "    seasonal_df = pd.concat([seasonal_df, season_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_df['DateTime'] = pd.to_datetime(seasonal_df['Date'] + ' ' + seasonal_df['Time'], infer_datetime_format=True, dayfirst=True, errors='coerce')\n",
    "seasonal_df = seasonal_df.sort_values(by='DateTime').reset_index(drop=True)\n",
    "# seasonal_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixtures = seasonal_df.copy()[(seasonal_df['FTHG'].isna()) & (seasonal_df['FTAG'].isna()) & (seasonal_df['FTR'].isna())][columns_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_df[['HGFH_r', 'HGCH_r']] = seasonal_df.groupby('HomeTeam', group_keys=False)[['FTHG', 'FTAG']].apply(lambda x: x.shift(shift_val).rolling(window=win_size, min_periods=1).mean())\n",
    "seasonal_df[['AGFA_r', 'AGCA_r']] = seasonal_df.groupby('AwayTeam', group_keys=False)[['FTAG', 'FTHG']].apply(lambda x: x.shift(shift_val).rolling(window=win_size, min_periods=1).mean())\n",
    "\n",
    "seasonal_df['HPtsH_r'] = seasonal_df.groupby('HomeTeam', group_keys=False)['HPointsH'].apply(lambda x: x.shift(shift_val).rolling(window=win_size, min_periods=1).mean())\n",
    "seasonal_df['APtsA_r'] = seasonal_df.groupby('AwayTeam', group_keys=False)['APointsA'].apply(lambda x: x.shift(shift_val).rolling(window=win_size, min_periods=1).mean())\n",
    "\n",
    "\n",
    "seasonal_df[['HGFH_e', 'HGCH_e']] = seasonal_df.groupby('HomeTeam', group_keys=False)[['FTHG', 'FTAG']].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "seasonal_df[['AGFA_e', 'AGCA_e']] = seasonal_df.groupby('AwayTeam', group_keys=False)[['FTAG', 'FTHG']].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "\n",
    "\n",
    "seasonal_df[['HSH_e', 'HSTH_e']] = seasonal_df.groupby('HomeTeam', group_keys=False)[['HS', 'HST']].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "seasonal_df[['ASA_e', 'ASTA_e']] = seasonal_df.groupby('AwayTeam', group_keys=False)[['AS', 'AST']].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "\n",
    "\n",
    "seasonal_df = seasonal_df.drop(['HPointsH','APointsA'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = sorted(set(seasonal_df['HomeTeam'].unique()) | set(seasonal_df['AwayTeam'].unique()))\n",
    "# seasons = seasonal_df.groupby(\"Season\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_cols = [\"GF\",\"GC\",\"SF\",\"SC\",\"STF\",\"STC\",\"Pts\"]\n",
    "\n",
    "for col in joint_cols:\n",
    "    seasonal_df[f\"H{col}\"] = 0\n",
    "    seasonal_df[f\"A{col}\"] = 0\n",
    "    # seasonal_df[f\"H{col}_r\"] = 0\n",
    "    # seasonal_df[f\"A{col}_r\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_cols = [\"Team\",\"GF\",\"GC\",\"SF\",\"SC\",\"STF\",\"STC\"]\n",
    "# for season_label, season_df in seasons:\n",
    "#     season_teams = sorted(set(season_df['HomeTeam'].unique()) | set(season_df['AwayTeam'].unique()))\n",
    "#     for team in season_teams:\n",
    "#         team_stats = pd.DataFrame(columns=joint_cols)\n",
    "#         team_homes = season_df[season_df[\"HomeTeam\"] == team][['HomeTeam','FTHG','FTAG','HS','AS','HST','AST']]\n",
    "#         team_aways = season_df[season_df[\"AwayTeam\"] == team][['AwayTeam','FTAG','FTHG','AS','HS','AST','HST']]\n",
    "#         team_homes = team_homes.rename(columns=dict(zip(team_homes.columns, joint_cols)))\n",
    "#         team_aways = team_aways.rename(columns=dict(zip(team_aways.columns, joint_cols)))\n",
    "\n",
    "#         team_stats = pd.concat([team_homes, team_aways])\n",
    "#         team_stats = team_stats.sort_index()\n",
    "        \n",
    "#         team_stats[['GF_m', 'GC_m']] = team_stats.groupby('Team', group_keys=False)[['GF', 'GC']].apply(lambda x: x.shift(shift_val).expanding().mean())\n",
    "#         team_stats['Points'] = team_stats.apply(lambda row: 3 if row['GF'] > row['GC'] else 1 if row['GF'] == row['GC'] else 0, axis=1)\n",
    "#         team_stats['Pts_m'] = team_stats.groupby('Team', group_keys=False)['Points'].apply(lambda x: x.shift(shift_val).expanding().mean())\n",
    "    \n",
    "    \n",
    "#         print(team_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in teams:\n",
    "    columns = [\"Team\",\"Opponent\"]+joint_cols\n",
    "    team_stats = pd.DataFrame(columns=columns)\n",
    "    team_homes = seasonal_df[seasonal_df[\"HomeTeam\"] == team][['HomeTeam','AwayTeam','FTHG','FTAG','HS','AS','HST','AST']]\n",
    "    team_aways = seasonal_df[seasonal_df[\"AwayTeam\"] == team][['AwayTeam','HomeTeam','FTAG','FTHG','AS','HS','AST','HST']]\n",
    "    team_homes = team_homes.rename(columns=dict(zip(team_homes.columns, columns)))\n",
    "    team_homes['HomeTeam'] = team_homes['Team']\n",
    "    team_homes['AwayTeam'] = team_homes['Opponent']\n",
    "    team_aways = team_aways.rename(columns=dict(zip(team_aways.columns, columns)))\n",
    "    team_aways['AwayTeam'] = team_homes['Team']\n",
    "    team_aways['HomeTeam'] = team_homes['Opponent']\n",
    "\n",
    "    team_stats = pd.concat([team_homes, team_aways])\n",
    "    team_stats = team_stats.sort_index()\n",
    "    \n",
    "    team_stats['Points'] = team_stats.apply(lambda row: 3 if row['GF'] > row['GC'] else 1 if row['GF'] == row['GC'] else 0, axis=1)\n",
    "    team_stats['Pts_r'] = team_stats.groupby('Team', group_keys=False)['Points'].apply(lambda x: x.shift(shift_val).rolling(window=win_size, min_periods=1).mean())\n",
    "    \n",
    "    team_stats[['GF_r', 'GC_r']] = team_stats.groupby('Team', group_keys=False)[['GF', 'GC']].apply(lambda x: x.shift(shift_val).rolling(window=win_size, min_periods=1).mean())\n",
    "    team_stats[['SF_r', 'STF_r', 'SC_r', 'STC_r']] = team_stats.groupby('Team', group_keys=False)[['SF', 'STF', 'SC', 'STC']].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "\n",
    "    team_stats[['GF_e', 'GC_e']] = team_stats.groupby('Team', group_keys=False)[['GF', 'GC']].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "    team_stats[['SF_e', 'STF_e', 'SC_e', 'STC_e']] = team_stats.groupby('Team', group_keys=False)[['SF', 'STF', 'SC', 'STC']].apply(lambda x: x.shift(shift_val).ewm(alpha=alpha, adjust=False).mean())\n",
    "    \n",
    "    merge_cols = ['Team','GF_e', 'GC_e', 'SF_e', 'SC_e', 'STF_e', 'STC_e', 'Pts_r']\n",
    "    merge_df = team_stats[merge_cols]\n",
    "    \n",
    "    mask_home_stats = (merge_df['Team'] == team_stats['HomeTeam'])\n",
    "    mask_away_stats = ~mask_home_stats\n",
    "    \n",
    "    home_cols = [f\"H{col}\" for col in joint_cols]\n",
    "    away_cols = [f\"A{col}\" for col in joint_cols]\n",
    "        \n",
    "    mask_home_df = mask_home_stats.reindex(seasonal_df.index, fill_value=False)\n",
    "    mask_away_df = mask_away_stats.reindex(seasonal_df.index, fill_value=False)\n",
    "\n",
    "    seasonal_home_idx = seasonal_df[mask_home_df].index\n",
    "    team_stats_home_idx = team_stats[mask_home_stats].index\n",
    "    seasonal_df.loc[seasonal_home_idx, home_cols] = team_stats.loc[team_stats_home_idx, merge_cols[1:]].values\n",
    "\n",
    "    seasonal_away_idx = seasonal_df[mask_away_df].index\n",
    "    team_stats_away_idx = team_stats[mask_away_stats].index\n",
    "    seasonal_df.loc[seasonal_away_idx, away_cols] = team_stats.loc[team_stats_away_idx, merge_cols[1:]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_with_avg_last_4(series):\n",
    "    return series.fillna(series.rolling(window=win_size, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_homes = seasonal_df.groupby(\"HomeTeam\")\n",
    "team_aways = seasonal_df.groupby(\"AwayTeam\")\n",
    "\n",
    "for name, group in team_homes:\n",
    "    seasonal_df.loc[group.index, group.columns.str.contains('_m')] = group.loc[:, group.columns.str.contains('_m')].apply(fill_nan_with_avg_last_4)\n",
    "\n",
    "for name, group in team_aways:\n",
    "    seasonal_df.loc[group.index, group.columns.str.contains('_m')] = group.loc[:, group.columns.str.contains('_m')].apply(fill_nan_with_avg_last_4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_drop_columns = result_cols + shot_cols + card_cols +['FTGD'] #'HTHG','HTAG','HTR',\n",
    "features_df = seasonal_df.dropna(subset=[col for col in seasonal_df.columns if col not in no_drop_columns]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['HoffStr'] = features_df['HGF'] + features_df['AGCA_e'] + features_df['HGFH_e'] + features_df['AGC']\n",
    "features_df['AoffStr'] = features_df['AGF'] + features_df['HGCH_e'] + features_df['AGFA_e'] + features_df['HGC']\n",
    "features_df['ShDiff'] = features_df['HSF'] + features_df['HSH_e'] - features_df['ASF'] - features_df['ASA_e']\n",
    "features_df['ShtDiff'] = features_df['HSTF'] + features_df['HSTH_e'] - features_df['ASTF'] - features_df['ASTA_e']\n",
    "\n",
    "features_df['GFdiff'] = features_df['HGF'] + features_df['HGFH_e'] - features_df['AGF'] - features_df['AGFA_e']\n",
    "features_df['GCdiff'] = features_df['HGC'] + features_df['HGCH_e'] - features_df['AGC'] - features_df['AGCA_e']\n",
    "\n",
    "features_df['PCH'] = features_df['HGFH_e'] + features_df['HSH_e'] + features_df['AGCA_e']\n",
    "features_df['PCA'] = features_df['AGFA_e'] + features_df['ASA_e'] + features_df['HGCH_e']\n",
    "\n",
    "features_df['PCdiff'] = features_df['PCH']  - features_df['PCA'] \n",
    "features_df['Ptsdiff'] = features_df['HPts']  - features_df['APts'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df['BTTS'] = features_df.apply(lambda row: 1 if row['FTHG'] > 0 and row['FTAG'] > 0 else 0, axis=1)\n",
    "\n",
    "# features_df['2more1'] = features_df.apply(lambda row: 1 if (row['FTHG'] + row['FTAG'] - row['HTHG'] + row['HTAG']) > (row['HTHG'] + row['HTAG']) else 0, axis=1)\n",
    "\n",
    "# features_df['over15'] = features_df.apply(lambda row: 1 if row['FTHG'] + row['FTAG'] > 1.5 else 0, axis=1)\n",
    "# features_df['over25'] = features_df.apply(lambda row: 1 if row['FTHG'] + row['FTAG'] > 2.5 else 0, axis=1)\n",
    "# features_df['over35'] = features_df.apply(lambda row: 1 if row['FTHG'] + row['FTAG'] > 3.5 else 0, axis=1)\n",
    "\n",
    "# features_df['Hover05'] = features_df.apply(lambda row: 1 if row['FTHG'] > 0.5 else 0, axis=1)\n",
    "# features_df['Hover15'] = features_df.apply(lambda row: 1 if row['FTHG'] > 1.5 else 0, axis=1)\n",
    "# features_df['Hover25'] = features_df.apply(lambda row: 1 if row['FTHG'] > 2.5 else 0, axis=1)\n",
    "# features_df['Hover35'] = features_df.apply(lambda row: 1 if row['FTHG'] > 3.5 else 0, axis=1)\n",
    "\n",
    "# features_df['Aover05'] = features_df.apply(lambda row: 1 if row['FTAG'] > 0.5 else 0, axis=1)\n",
    "# features_df['Aover15'] = features_df.apply(lambda row: 1 if row['FTAG'] > 1.5 else 0, axis=1)\n",
    "# features_df['Aover25'] = features_df.apply(lambda row: 1 if row['FTAG'] > 2.5 else 0, axis=1)\n",
    "# features_df['Aover35'] = features_df.apply(lambda row: 1 if row['FTAG'] > 3.5 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['H'] = (features_df['FTR'] == 'H').astype(int)\n",
    "features_df['D'] = (features_df['FTR'] == 'D').astype(int)\n",
    "features_df['A'] = (features_df['FTR'] == 'A').astype(int)\n",
    "\n",
    "\n",
    "sns.heatmap(features_df[['HSH_m','HPtsH_m','HPts','HGF','HGFH_e','AGC','HoffStr','HSH_e','HSF','PCH','FTHG']].corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.show() #HPtsH_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(features_df[['ASA_m','APtsA_m','APts','AGF','AGFA_e','HGCH_e','HGC','HGCH_e','ASA_e','ASF','PCA','FTAG']].corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.show() #APtsA_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(features_df[['ShDiff','ShtDiff', 'GFdiff','GCdiff','PCdiff','Ptsdiff','HoffStr','AoffStr','FTGD']].corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.show() #APtsA_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_cols = [col for col in features_df.columns.tolist() if 'over' in col]\n",
    "\n",
    "game_infos =['Div','DateTime','HomeTeam','AwayTeam'] + result_cols + card_cols # 'BTTS','2more1']\n",
    "\n",
    "input_features  = ['HGF','HGFH_e','AGC','AGCA_e','HSH_e','HSF', 'HPts', #'HPtsH_m', 'HPts', 'PCH',\n",
    "                   'AGF','AGFA_e','HGC','HGCH_e','ASA_e','ASF', 'APts',#'APtsA_m', 'APts','PCA',\n",
    "                #    'ShtDiff','GFdiff'#'ShDiff','HoffStr', 'AoffStr',\n",
    "                   ]\n",
    "\n",
    "# input_features  = ['HoffStr','HGFH_e','HSF', #'HPtsH_m', 'HPts', 'PCH',\n",
    "#                    'AoffStr','AGFA_e','ASF', #'APtsA_m', 'APts','PCA',\n",
    "#                    'ShDiff','ShtDiff', 'GFdiff'#,'GCdiff'\n",
    "#                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = game_infos + over_cols + input_features + odds\n",
    "\n",
    "\n",
    "data = features_df.copy()[selected_features]\n",
    "data['R'] = data['FTR'].map({'H': 1, 'D': 0, 'A': 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'R'\n",
    "\n",
    "test_size = 311#len(leagues_keys)*36\n",
    "\n",
    "X_train,y_train,meta_train,X_test,y_test,meta_test,X_pred,meta_pred = split_data(data,fixtures,test_size,game_infos,over_cols,odds,target, no_red = True)\n",
    "X_train_scaled,X_test_scaled,X_pred_scaled = scale_datasets(X_train,X_test,X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(y_test)\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(2**4, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(2**5, activation='relu'),\n",
    "    # layers.Dense(2**3, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Output layer with softmax for classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_scaled, keras.utils.to_categorical(y_train, num_classes=num_classes),\n",
    "        epochs=35, batch_size=2**5, validation_data=(X_test_scaled, keras.utils.to_categorical(y_test, num_classes=num_classes)))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_scaled, keras.utils.to_categorical(y_test, num_classes=num_classes))\n",
    "print(f'Loss on Test Set: {loss}')\n",
    "print(f'Accuracy on Test Set: {accuracy}')\n",
    "\n",
    "\n",
    "# num_classes = 1  # For binary classification, you only need one output neuron\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(2**5, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "#     layers.Dense(2**4, activation='gelu'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train_scaled, y_train, epochs=50, batch_size=2**5, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "# print(f'Loss on Test Set: {loss}')\n",
    "# print(f'Accuracy on Test Set: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_layer_weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# feature_importances = np.sum(np.abs(first_layer_weights), axis=1)\n",
    "\n",
    "# feature_importances /= np.sum(feature_importances)\n",
    "\n",
    "# sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# sorted_feature_names = [input_features[i] for i in sorted_indices]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(range(X_train.shape[1]), feature_importances[sorted_indices])\n",
    "# plt.xticks(range(X_train.shape[1]), sorted_feature_names, rotation=45, ha='right')  # Rotate feature names for better visibility\n",
    "# plt.xlabel('Feature')\n",
    "# plt.ylabel('Normalized Importance')\n",
    "# plt.title('Feature Importances in the First Layer')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_probabilities = model.predict(X_pred_scaled)\n",
    "\n",
    "probh_ann = ann_probabilities[:, 1]\n",
    "probd_ann = ann_probabilities[:, 0]\n",
    "proba_ann = ann_probabilities[:, 2]\n",
    "\n",
    "# result_df_ann = table(probh_ann,probd_ann,proba_ann,meta_test,meta_pred,min_entropy,max_entropy)\n",
    "# confusion_classification(result_df_ann, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_bet_df = table_bet(result_df_ann,bet_amount_base)\n",
    "\n",
    "# total_hypothetical_winnings = ann_bet_df['result'].head(test_size).sum()\n",
    "# print(f\"Total Hypothetical Winnings: {round(total_hypothetical_winnings,2)} euros over {test_size} games (with total bet : {ann_bet_df['base_bet'].sum()}€)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_table = table_pred(result_df_ann,len(fixtures))\n",
    "# np.round(pred_table.head(10),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.round(pred_table.sort_values('Odd').head(3),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_table_sorted = np.round(pred_table.sort_values('Bprob',ascending = False).head(3),2)\n",
    "# pred_table_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_table_sorted.base_bet.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_fit(X_train_scaled,y_train):\n",
    "    \n",
    "    mlp_model = MLPClassifier(  activation='relu', \n",
    "                                max_iter=800, \n",
    "                                # power_t=0.5, \n",
    "                                # validation_fraction=0.1, \n",
    "                                # beta_1=0.9, beta_2=0.999,\n",
    "                                # epsilon=1e-08,\n",
    "                                \n",
    "                                random_state=42)\n",
    "    \n",
    "    # Create the parameter grid for grid search\n",
    "    param_grid = {'hidden_layer_sizes':[(2**4,2**3)], #(2**3, 2**3),(2**5, 2**3),\n",
    "                  } #0.005,\n",
    "\n",
    "    # Perform grid search to find the best hyperparameters for Random Forest\n",
    "    grid_search_mlp = GridSearchCV(mlp_model, param_grid, cv=5, scoring='accuracy', verbose=3, return_train_score=True)\n",
    "    grid_search_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Print the best parameters\n",
    "    print(\"Best Parameters for MLP:\")\n",
    "    print(grid_search_mlp.best_params_)\n",
    "    \n",
    "    # Train the model on the best hyperparameters found using grid search\n",
    "    best_mlp = grid_search_mlp.best_estimator_\n",
    "    \n",
    "\n",
    "    return best_mlp.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = mlp_model_fit(X_train_scaled,y_train)\n",
    "mlp_probabilities =  mlp_model.predict_proba(X_pred_scaled)\n",
    "accuracy_mlp = accuracy_score(y_test, np.argmax(mlp_probabilities[:test_size], axis=1))\n",
    "print(f'Accuracy on Test Set (MLP): {accuracy_mlp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probh_mlp = mlp_probabilities[:, 1]\n",
    "probd_mlp = mlp_probabilities[:, 0]\n",
    "proba_mlp = mlp_probabilities[:, 2]\n",
    "\n",
    "# result_df_mlp = table(probh_mlp,probd_mlp,proba_mlp,meta_test,meta_pred,min_entropy,max_entropy)\n",
    "# confusion_classification(result_df_mlp, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_bet_df = table_bet(result_df_mlp,bet_amount_base)\n",
    "\n",
    "# total_hypothetical_winnings = mlp_bet_df['result'].head(test_size).sum()\n",
    "# print(f\"Total Hypothetical Winnings: {round(total_hypothetical_winnings,2)} euros over {test_size} games (with total bet : {mlp_bet_df['base_bet'].sum()}€)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_table = table_pred(result_df_mlp,len(fixtures))\n",
    "# np.round(pred_table.head(10),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for Random Forest\n",
    "params = {\n",
    "    'n_estimators':90,\n",
    "    'criterion': 'gini',\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,'max_features': 'sqrt','max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'oob_score': False,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'verbose': 0,'warm_start': False, 'class_weight': None,\n",
    "}\n",
    "\n",
    "# Create the Random Forest classifier\n",
    "rfc = RandomForestClassifier(**params)\n",
    "\n",
    "# Create the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'max_depth': [20]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters for Random Forest\n",
    "grid_search_rfc = GridSearchCV(rfc, param_grid, cv=5, scoring='accuracy', verbose=1, return_train_score=True)\n",
    "grid_search_rfc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train the model on the best hyperparameters found using grid search\n",
    "best_rfc = grid_search_rfc.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "print(\"Best Parameters for RFC:\")\n",
    "print(grid_search_rfc.best_params_)\n",
    "\n",
    "best_rfc.fit(X_train_scaled, y_train)\n",
    "\n",
    "calibrated_model = CalibratedClassifierCV(best_rfc, cv='prefit', method='isotonic')\n",
    "calibrated_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_rfc = calibrated_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy_rfc = accuracy_score(y_test, predictions_rfc)\n",
    "print(\"Accuracy on test set: \", accuracy_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_probabilities = calibrated_model.predict_proba(X_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probh_rfc,probd_rfc,proba_rfc = rfc_probabilities[:, 1],rfc_probabilities[:, 0],rfc_probabilities[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "### Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "coef = 2\n",
    "\n",
    "probh_square = probh_mlp**coef+probh_rfc**coef+probh_ann**coef\n",
    "probd_square = probd_mlp**coef+probd_rfc**coef+probd_ann**coef\n",
    "proba_square = proba_mlp**coef+proba_rfc**coef+proba_ann**coef\n",
    "\n",
    "combined_square = np.column_stack((proba_square, probd_square, probh_square))\n",
    "combined_square = scaler.fit_transform(combined_square)\n",
    "\n",
    "bookiesh = 1/features_df[-(test_size+len(fixtures)):]['PSH']\n",
    "bookiesd = 1/features_df[-(test_size+len(fixtures)):]['PSD']\n",
    "bookiesa = 1/features_df[-(test_size+len(fixtures)):]['PSA']\n",
    "combined_books= np.column_stack((proba_square, probd_square, probh_square))\n",
    "combined_books = scaler.fit_transform(combined_books)\n",
    "\n",
    "# square_preds = np.argmax(square_norms, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = np.column_stack((ann_probabilities, mlp_probabilities, rfc_probabilities,combined_square))#combined_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_last = len(leagues_keys)*20\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(preds_test[:test_size], y_test)\n",
    "\n",
    "logis_preds = lr_model.predict(preds_test)\n",
    "logis_proba = lr_model.predict_proba(preds_test)\n",
    "\n",
    "probh = logis_proba[:,1]\n",
    "probd = logis_proba[:,0]\n",
    "proba = logis_proba[:,2]\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, logis_preds[:test_size])\n",
    "accuracy_last10 = accuracy_score(y_test[-acc_last:], logis_preds[test_size-acc_last:test_size])\n",
    "\n",
    "f\"accuracy last {test_size} : {accuracy_lr}, accuracy last {acc_last} : {accuracy_last10}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 0.055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_margin , accuracy_margin = custom_accuracy_margin(y_test, logis_proba[:test_size], threshold=threshold_value)\n",
    "df_double_chance_strict , accuracy_double = accuracy_double_strict(y_test, logis_proba[:test_size], threshold=threshold_value)\n",
    "df_double_chance_margin , accuracy_double_marg = accuracy_double_margin(y_test, logis_proba[:test_size], threshold=threshold_value)\n",
    "print(f\"accuracy_margin with threshold {threshold_value}: {accuracy_margin}\")\n",
    "print(f\"accuracy_double without margin {threshold_value}: {accuracy_double}\")\n",
    "print(f\"accuracy_double_margin with threshold {threshold_value}: {accuracy_double_marg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = (df['Diff Condition 1'] == True) | (df['Diff Condition 2'] == True)\n",
    "# condition = (df['True Class'] == df['Second Prediction'])\n",
    "# condition = (df['True Class'] != df['Predicted Class']) & (df['True Class'] != df['Second Prediction'])\n",
    "\n",
    "# df[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = lr_model.coef_[0]\n",
    "feature_names = ['annh','annd','anna', 'mlph','mlpd','mlpa', 'rfch','rfcd','rfca','squareh','squared','squarea']\n",
    "\n",
    "# Create a dictionary to associate feature names with their coefficients\n",
    "feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the features by their absolute coefficient values to see the most influential ones\n",
    "sorted_features = sorted(feature_coefficients.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Display the sorted features and their coefficients\n",
    "for feature, coefficient in sorted_features:\n",
    "    print(f\"{feature}: {coefficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = table(probh,probd,proba,meta_test,meta_pred,min_entropy,max_entropy).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(result.tail(10),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_classification(result, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bet_df = table_bet(result,bet_amount_base)\n",
    "\n",
    "total_hypothetical_winnings = result_bet_df['result'].head(test_size).sum()\n",
    "print(f\"Total Hypothetical Winnings: {round(total_hypothetical_winnings,2)} euros over {len(result_bet_df[:test_size])} games (with total bet : {result_bet_df[:test_size]['base_bet'].sum()}€)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_table = table_pred(result,len(meta_pred))\n",
    "pred_table['|'] = \"|\"\n",
    "pred_table['Hodd'] = 1/pred_table['Home']\n",
    "pred_table['Dodd'] = 1/pred_table['Draw']\n",
    "pred_table['Aodd'] = 1/pred_table['Away']\n",
    "\n",
    "size_table = len(pred_table)\n",
    "np.round(pred_table.head(size_table//2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(pred_table.tail(size_table//2),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--clermont n2\n",
    "--metz n2\n",
    "--everton\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
